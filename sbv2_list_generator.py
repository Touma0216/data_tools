#!/usr/bin/env python3
"""
Style-Bert-VITS2 ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ„ãƒ¼ãƒ«
metadata.jsonã‹ã‚‰esd.list, train.list, val.listã‚’è‡ªå‹•ç”Ÿæˆ
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import random
from datetime import datetime
import shutil

class SBV2ListGenerator:
    """Style-Bert-VITS2ç”¨ã®ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str = "reineHonoka"):
        self.model_name = model_name
        self.metadata = {}
        self.file_list = []
        
    def load_metadata(self, metadata_path: Path) -> bool:
        """metadata.jsonã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.metadata)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
            return True
        except Exception as e:
            print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return False
    
    def validate_metadata(self) -> List[str]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        errors = []
        required_fields = ['text', 'speaker', 'language']
        
        for filename, data in self.metadata.items():
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            for field in required_fields:
                if field not in data:
                    errors.append(f"{filename}: '{field}'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
            if not filename.endswith(('.wav', '.mp3', '.flac', '.ogg')):
                errors.append(f"{filename}: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„éŸ³å£°å½¢å¼")
        
        if errors:
            print("âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å•é¡ŒãŒã‚ã‚Šã¾ã™:")
            for error in errors[:10]:  # æœ€åˆã®10å€‹ã¾ã§è¡¨ç¤º
                print(f"  - {error}")
            if len(errors) > 10:
                print(f"  ... ä»– {len(errors) - 10} å€‹ã®ã‚¨ãƒ©ãƒ¼")
        
        return errors
    
    def generate_esd_list(self, output_dir: Path) -> bool:
        """esd.listã‚’ç”Ÿæˆ"""
        esd_lines = []
        
        for filename, data in sorted(self.metadata.items()):
            # åŸºæœ¬æƒ…å ±ã®å–å¾—
            speaker = data.get('speaker', self.model_name)
            language = data.get('language', 'JP')
            text = data.get('text', '')
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ï¼ˆæ”¹è¡Œã‚„ã‚¿ãƒ–ã‚’é™¤å»ï¼‰
            text = text.replace('\n', ' ').replace('\t', ' ').strip()
            
            # esd.listå½¢å¼: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹|è©±è€…å|è¨€èª|ãƒ†ã‚­ã‚¹ãƒˆ
            line = f"{filename}|{speaker}|{language}|{text}"
            esd_lines.append(line)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        esd_path = output_dir / 'esd.list'
        try:
            with open(esd_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(esd_lines))
            print(f"âœ… esd.listã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(esd_lines)}è¡Œ")
            return True
        except Exception as e:
            print(f"âŒ esd.listç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def split_train_val(self, val_ratio: float = 0.1) -> Tuple[List[str], List[str]]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²"""
        all_files = list(self.metadata.keys())
        
        # æ„Ÿæƒ…ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆãƒãƒ©ãƒ³ã‚¹ã‚’ä¿ã¤ãŸã‚ï¼‰
        emotion_groups = {}
        for filename in all_files:
            emotion = self.metadata[filename].get('emotion', 'neutral')
            if emotion not in emotion_groups:
                emotion_groups[emotion] = []
            emotion_groups[emotion].append(filename)
        
        train_files = []
        val_files = []
        
        # å„æ„Ÿæƒ…ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ¯”ç‡ã«å¿œã˜ã¦åˆ†å‰²
        for emotion, files in emotion_groups.items():
            random.shuffle(files)
            val_count = max(1, int(len(files) * val_ratio))
            val_files.extend(files[:val_count])
            train_files.extend(files[val_count:])
        
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        random.shuffle(train_files)
        random.shuffle(val_files)
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: å­¦ç¿’ç”¨ {len(train_files)}å€‹ / æ¤œè¨¼ç”¨ {len(val_files)}å€‹")
        
        # æ„Ÿæƒ…åˆ†å¸ƒã‚’è¡¨ç¤º
        print("\næ„Ÿæƒ…åˆ†å¸ƒ:")
        for emotion in emotion_groups.keys():
            train_emotion = sum(1 for f in train_files if self.metadata[f].get('emotion', 'neutral') == emotion)
            val_emotion = sum(1 for f in val_files if self.metadata[f].get('emotion', 'neutral') == emotion)
            print(f"  {emotion}: å­¦ç¿’ç”¨ {train_emotion}å€‹ / æ¤œè¨¼ç”¨ {val_emotion}å€‹")
        
        return train_files, val_files
    
    def generate_train_val_lists(self, output_dir: Path, val_ratio: float = 0.1) -> bool:
        """train.listã¨val.listã‚’ç”Ÿæˆ"""
        train_files, val_files = self.split_train_val(val_ratio)
        
        # train.listç”Ÿæˆ
        train_lines = []
        for filename in train_files:
            data = self.metadata[filename]
            speaker = data.get('speaker', self.model_name)
            language = data.get('language', 'JP')
            text = data.get('text', '').replace('\n', ' ').replace('\t', ' ').strip()
            
            # æ³¨: éŸ³ç´ åˆ—ã¯å‰å‡¦ç†æ™‚ã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯åŸºæœ¬å½¢å¼ã®ã¿
            line = f"wavs/{filename}|{speaker}|{language}|{text}"
            train_lines.append(line)
        
        # val.listç”Ÿæˆ
        val_lines = []
        for filename in val_files:
            data = self.metadata[filename]
            speaker = data.get('speaker', self.model_name)
            language = data.get('language', 'JP')
            text = data.get('text', '').replace('\n', ' ').replace('\t', ' ').strip()
            
            line = f"wavs/{filename}|{speaker}|{language}|{text}"
            val_lines.append(line)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        try:
            train_path = output_dir / 'train.list'
            with open(train_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(train_lines))
            print(f"âœ… train.listã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(train_lines)}è¡Œ")
            
            val_path = output_dir / 'val.list'
            with open(val_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(val_lines))
            print(f"âœ… val.listã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(val_lines)}è¡Œ")
            
            return True
        except Exception as e:
            print(f"âŒ train/val.listç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_extended_list(self, output_dir: Path) -> bool:
        """æ‹¡å¼µæƒ…å ±ã‚’å«ã‚€ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
        extended_lines = []
        
        for filename, data in sorted(self.metadata.items()):
            # å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            speaker = data.get('speaker', self.model_name)
            language = data.get('language', 'JP')
            text = data.get('text', '').replace('\n', ' ').replace('\t', ' ').strip()
            emotion = data.get('emotion', 'neutral')
            emotion_strength = data.get('emotion_strength', 1.0)
            style = data.get('style', 'default')
            speaker_note = data.get('speaker_note', '')
            situation_context = data.get('situation_context', '')
            
            # æ‹¡å¼µå½¢å¼
            line = f"{filename}|{speaker}|{language}|{text}|{emotion}|{emotion_strength}|{style}|{speaker_note}|{situation_context}"
            extended_lines.append(line)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        extended_path = output_dir / 'esd_extended.list'
        try:
            with open(extended_path, 'w', encoding='utf-8') as f:
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è¿½åŠ 
                header = "# filename|speaker|language|text|emotion|emotion_strength|style|speaker_note|situation_context"
                f.write(header + '\n')
                f.write('\n'.join(extended_lines))
            print(f"âœ… esd_extended.listã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(extended_lines)}è¡Œ")
            return True
        except Exception as e:
            print(f"âŒ extended.listç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_style_config(self, output_dir: Path) -> bool:
        """ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        # æ„Ÿæƒ…ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®çµ±è¨ˆã‚’åé›†
        emotions = {}
        styles = {}
        
        for data in self.metadata.values():
            emotion = data.get('emotion', 'neutral')
            style = data.get('style', 'default')
            
            emotions[emotion] = emotions.get(emotion, 0) + 1
            styles[style] = styles.get(style, 0) + 1
        
        style_config = {
            "model_name": self.model_name,
            "emotions": emotions,
            "styles": styles,
            "total_files": len(self.metadata),
            "generated_at": datetime.now().isoformat()
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        config_path = output_dir / 'style_config.json'
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(style_config, f, ensure_ascii=False, indent=2)
            print(f"âœ… style_config.jsonã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
            return True
        except Exception as e:
            print(f"âŒ style_config.jsonç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def backup_existing_lists(self, output_dir: Path):
        """æ—¢å­˜ã®ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        backup_dir = output_dir / 'backup' / datetime.now().strftime('%Y%m%d_%H%M%S')
        
        list_files = ['esd.list', 'train.list', 'val.list', 'esd_extended.list']
        files_to_backup = []
        
        for file in list_files:
            file_path = output_dir / file
            if file_path.exists():
                files_to_backup.append(file_path)
        
        if files_to_backup:
            backup_dir.mkdir(parents=True, exist_ok=True)
            for file_path in files_to_backup:
                shutil.copy2(file_path, backup_dir / file_path.name)
            print(f"ğŸ“ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ: {backup_dir}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("\n=== Style-Bert-VITS2 ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ„ãƒ¼ãƒ« ===")
    print("metadata.jsonã‹ã‚‰å„ç¨®ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™\n")
    
    # è¨­å®š
    model_name = input("ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ› (default: reineHonoka): ").strip() or "reineHonoka"
    
    # ãƒ‘ã‚¹ã®è¨­å®š
    base_dir = Path(f"Data/{model_name}")
    metadata_path = base_dir / "metadata.json"
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
    if not metadata_path.exists():
        print(f"âŒ metadata.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_path}")
        
        # ã‚µãƒ³ãƒ—ãƒ«metadata.jsonã‚’ç”Ÿæˆ
        print("\nã‚µãƒ³ãƒ—ãƒ«ã®metadata.jsonã‚’ç”Ÿæˆã—ã¾ã™ã‹ï¼Ÿ (y/n): ", end="")
        if input().lower() == 'y':
            create_sample_metadata(base_dir)
            print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {metadata_path}")
            print("ç·¨é›†ã—ã¦ã‹ã‚‰å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
    generator = SBV2ListGenerator(model_name)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if not generator.load_metadata(metadata_path):
        return
    
    # æ¤œè¨¼
    errors = generator.validate_metadata()
    if errors:
        print("\nç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ", end="")
        if input().lower() != 'y':
            return
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    generator.backup_existing_lists(base_dir)
    
    # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡
    val_ratio_str = input("\næ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡ (default: 0.1): ").strip()
    val_ratio = float(val_ratio_str) if val_ratio_str else 0.1
    
    # ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
    print("\nğŸ“ ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...")
    
    # esd.list
    generator.generate_esd_list(base_dir)
    
    # train.list / val.list
    generator.generate_train_val_lists(base_dir, val_ratio)
    
    # æ‹¡å¼µãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    print("\næ‹¡å¼µãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç”Ÿæˆã—ã¾ã™ã‹ï¼Ÿ (y/n): ", end="")
    if input().lower() == 'y':
        generator.generate_extended_list(base_dir)
    
    # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
    generator.generate_style_config(base_dir)
    
    print("\nâœ¨ å®Œäº†ï¼")
    print(f"ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ {base_dir} ã«ã‚ã‚Šã¾ã™")


def create_sample_metadata(base_dir: Path):
    """ã‚µãƒ³ãƒ—ãƒ«ã®metadata.jsonã‚’ä½œæˆ"""
    base_dir.mkdir(parents=True, exist_ok=True)
    
    sample_metadata = {
        "001.wav": {
            "text": "ã“ã‚“ã«ã¡ã¯ã€é›¶éŸ³ã»ã®ã‹ã§ã™",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "happy",
            "emotion_strength": 0.8,
            "style": "cheerful",
            "speaker_note": "<Parse_happy>æ˜ã‚‹ãå…ƒæ°—ãªæŒ¨æ‹¶",
            "situation_context": "é…ä¿¡é–‹å§‹ã®æŒ¨æ‹¶"
        },
        "002.wav": {
            "text": "ä»Šæ—¥ã‚‚ä¸€ç·’ã«é ‘å¼µã‚Šã¾ã—ã‚‡ã†",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "happy",
            "emotion_strength": 0.7,
            "style": "encouraging",
            "speaker_note": "<Parse_happy>åŠ±ã¾ã—ã®ãƒˆãƒ¼ãƒ³",
            "situation_context": "ãƒªã‚¹ãƒŠãƒ¼ã¸ã®å¿œæ´"
        },
        "003.wav": {
            "text": "ã©ã†ã—ã¦ã“ã‚“ãªã“ã¨ã«...",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "sad",
            "emotion_strength": 0.9,
            "style": "depressed",
            "speaker_note": "<Parse_sad>æ·±ã„æ‚²ã—ã¿ã‚’è¾¼ã‚ã¦",
            "situation_context": "ã‚²ãƒ¼ãƒ ã§å¤§å¤±æ•—ã—ãŸæ™‚"
        },
        "004.wav": {
            "text": "ã¡ã‚‡ã£ã¨ã€ãã‚Œã¯ãªã„ã§ã—ã‚‡ï¼",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "angry",
            "emotion_strength": 0.6,
            "style": "frustrated",
            "speaker_note": "<Parse_angry>è»½ã„æ€’ã‚Šã¨å‘†ã‚Œ",
            "situation_context": "ç†ä¸å°½ãªå±•é–‹ã¸ã®åå¿œ"
        },
        "005.wav": {
            "text": "ãˆã£ã¨...ãã®...ãªã‚“ã¨ã„ã†ã‹...",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "embarrassed",
            "emotion_strength": 0.8,
            "style": "shy",
            "speaker_note": "<Parse_embarrassed>æ¥ãšã‹ã—ãã†ã«",
            "situation_context": "è¤’ã‚ã‚‰ã‚ŒãŸæ™‚ã®åå¿œ"
        },
        "006.wav": {
            "text": "æœ¬å½“ã«æ€–ã„ã‚“ã ã£ã¦ï¼",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "fear",
            "emotion_strength": 0.95,
            "style": "terrified",
            "speaker_note": "<Parse_fear>éœ‡ãˆå£°ã§",
            "situation_context": "ãƒ›ãƒ©ãƒ¼ã‚²ãƒ¼ãƒ å®Ÿæ³ä¸­"
        },
        "007.wav": {
            "text": "ã‚„ã£ãŸãƒ¼ï¼ã¤ã„ã«æˆåŠŸã—ãŸï¼",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "happy",
            "emotion_strength": 1.0,
            "style": "excited",
            "speaker_note": "<Parse_happy>å¤§å–œã³ã§å«ã¶",
            "situation_context": "é›£ã—ã„ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªã‚¢"
        },
        "008.wav": {
            "text": "ã¿ã‚“ãªã€æœ¬å½“ã«ã‚ã‚ŠãŒã¨ã†",
            "speaker": "reineHonoka",
            "language": "JP",
            "emotion": "grateful",
            "emotion_strength": 0.85,
            "style": "heartfelt",
            "speaker_note": "<Parse_grateful>å¿ƒã‚’è¾¼ã‚ã¦",
            "situation_context": "é…ä¿¡çµ‚äº†æ™‚ã®æ„Ÿè¬"
        }
    }
    
    metadata_path = base_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(sample_metadata, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()